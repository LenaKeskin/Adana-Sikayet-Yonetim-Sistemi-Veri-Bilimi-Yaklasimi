# -*- coding: utf-8 -*-
"""adana_sikayet_gelismis_ml_ve_sistem_entegrasyonu.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19tN7svijk2PmRvm1JPRTLvZmG1oAvial

Bu bÃ¶lÃ¼m, "ADANA ÅÄ°KAYET YÃ–NETÄ°M SÄ°STEMÄ°:VERÄ° BÄ°LÄ°MÄ° YAKLAÅIMI PROJESÄ°" kapsamlÄ± ve operasyonel bir dÃ¼zeye taÅŸÄ±maktadÄ±r. GeniÅŸletilmiÅŸ SQL veri setleriyle, XGBoost ve Naive Bayes gibi makine Ã¶ÄŸrenimi modelleri entegre edilmiÅŸtir.Sistem, otomasyon ve Flask tabanlÄ± bir web arayÃ¼zÃ¼ ile daha akÄ±llÄ±, otomatik ve eriÅŸilebilir bir ÅŸikayet yÃ¶netim Ã§Ã¶zÃ¼mÃ¼ sunmayÄ± hedeflemektedir.Ek olarak AyrÄ±ca, TensorFlow/Keras ile ÅŸikayet metinlerinden kategori tahmini gibi ileri dÃ¼zey doÄŸal dil iÅŸleme (NLP) yetenekleri kazandÄ±rÄ±lmÄ±ÅŸtÄ±r.

Dosya YÃ¼kleme Ä°ÅŸleme
"""

from google.colab import files
uploaded = files.upload()

"""YÃ¼kleme YaptÄ±ktan Sonra Ã–rnek Okuma Ä°ÅŸlemi"""

import pandas as pd

# Dosya adÄ±nÄ± seninkine gÃ¶re deÄŸiÅŸtir
df = pd.read_csv("Sikayet_1.csv", sep=';')

# Ä°lk 15 satÄ±rÄ± tablo halinde gÃ¶ster (print kullanmayÄ±n)
df.head(15)

"""Veri Ã–n Ä°ÅŸleme"""

import pandas as pd

# CSV dosyasÄ±nÄ± yÃ¼kle (dosya adÄ±nÄ± kendi dosyana gÃ¶re deÄŸiÅŸtir)
df = pd.read_csv("Sikayet_1.csv", sep=';')

# Ä°lk birkaÃ§ satÄ±rÄ± gÃ¶ster (kontrol iÃ§in)
print("Ä°lk 5 SatÄ±r:")
display(df.head())

# 1. Åekil bozukluÄŸu olan sÃ¼tunlarÄ± dÃ¼zelt (gerekiyorsa sÃ¼tun adlarÄ±nÄ± kontrol et)
df.columns = df.columns.str.strip()  # baÅŸtaki ve sondaki boÅŸluklarÄ± temizle

# 2. Eksik verileri kontrol et
print("Eksik DeÄŸer SayÄ±sÄ± (Her Kolonda):")
print(df.isnull().sum())

# 3. Eksik deÄŸerleri temizleme veya doldurma
# - SayÄ±sal kolonlarda ortalama ile doldurma Ã¶rneÄŸi
if 'Cozum_Suresi_gun' in df.columns:
    df['Cozum_Suresi_gun'].fillna(df['Cozum_Suresi_gun'].mean(), inplace=True)

# - Kategorik kolonlarda mod (en sÄ±k geÃ§en) ile doldurma Ã¶rneÄŸi
for col in ['Ä°lÃ§e', 'Kategori', 'Kanal', 'Cozum_Durumu']:
    if col in df.columns:
        df[col].fillna(df[col].mode()[0], inplace=True)

# 4. Tekrarlayan satÄ±r var mÄ±?
print("Tekrarlayan SatÄ±r SayÄ±sÄ±:")
print(df.duplicated().sum())

# 5. Gerekirse tekrar eden satÄ±rlarÄ± kaldÄ±r
df.drop_duplicates(inplace=True)

# 6. Tarih sÃ¼tunu varsa, datetime formatÄ±na Ã§evir
if 'Tarih' in df.columns:
    df['Tarih'] = pd.to_datetime(df['Tarih'], errors='coerce')

# 7. GÃ¼ncellenmiÅŸ tabloya genel bakÄ±ÅŸ
print("GÃ¼ncellenmiÅŸ Veri Ã–zeti:")
display(df.info())
display(df.head())

"""Veri DÃ¶nÃ¼ÅŸtÃ¼rme ve Yeni Ã–zellik Ekleme (Feature Engineering)"""

import pandas as pd
from datetime import datetime

# Tarih sÃ¼tununu datetime formatÄ±na Ã§evir
df["Tarih"] = pd.to_datetime(df["Tarih"], dayfirst=True)

# 1. GÃ¼n ismi (Pazartesi, SalÄ±â€¦)
df["Gun"] = df["Tarih"].dt.day_name()

# 2. Ã‡Ã¶zÃ¼m SÃ¼resine gÃ¶re Cozum_Hizi
def hiz_sinifla(gun):
    if gun <= 2:
        return "HÄ±zlÄ±"
    elif gun <= 7:
        return "Orta"
    else:
        return "GeÃ§"
df["Cozum_Hizi"] = df["Cozum_Suresi_gÃ¼n"].apply(hiz_sinifla)

# 3. Cevap Var mÄ±? (BaÅŸvuru sayÄ±sÄ± > 0 ise 1, yoksa 0)
df["Cevap_Var"] = df["Sikayet_basvuru_sayisi"].apply(lambda x: 1 if x > 0 else 0)

# 4. Ä°lÃ§e Ã¶zel sÄ±ralamasÄ±
df["Ilce_Sikayet_Sirasi"] = df.groupby("Ä°lÃ§e")["Tarih"].rank().astype(int)

# 5. Ä°lÃ§eye gÃ¶re yoÄŸunluk seviyesi (Toplam baÅŸvuruya gÃ¶re)
ilce_toplam = df.groupby("Ä°lÃ§e")["Sikayet_basvuru_sayisi"].sum()
ilce_yogunluk = pd.cut(ilce_toplam, bins=3, labels=["DÃ¼ÅŸÃ¼k", "Orta", "YÃ¼ksek"])
ilce_yogunluk = ilce_yogunluk.to_dict()
df["Yogunluk_Seviyesi"] = df["Ä°lÃ§e"].map(ilce_yogunluk)

# 6. Cozum Var mÄ±? (Durum = "Ã‡Ã¶zÃ¼ldÃ¼" ise 1)
df["Cozum_Var"] = df["Cozum_Durumu"].apply(lambda x: 1 if str(x).strip().lower() == "Ã§Ã¶zÃ¼ldÃ¼" else 0)

# 7. Zaman Dilimi (Sabah / Ã–ÄŸle / AkÅŸam)
df["Saat"] = df["Tarih"].dt.hour
def zaman_dilimi(saat):
    if saat < 12:
        return "Sabah"
    elif saat < 18:
        return "Ã–ÄŸle"
    else:
        return "AkÅŸam"
df["Zaman_Dilimi"] = df["Saat"].apply(zaman_dilimi)
df.drop("Saat", axis=1, inplace=True)

# 8. Haftasonu mu?
df["Haftasonu_mu"] = df["Tarih"].dt.weekday.apply(lambda x: 1 if x >= 5 else 0)

# 9. Åikayet AÃ§Ä±klamasÄ± UzunluÄŸu
df["Sikayet_Uzunlugu"] = df["Detay"].apply(lambda x: len(str(x)))

# Yeni halini kontrol et
df.head(15)

"""Temel Ä°statistikler"""

print("\nÃ‡Ã¶zÃ¼m SÃ¼resi Ä°statistikleri (GÃ¼n):")
print(df['Cozum_Suresi_gÃ¼n'].describe())

print("\nÃ‡Ã¶zÃ¼m HÄ±zÄ± Ä°statistikleri (0-1 arasÄ±):")
print(df['Cozum_Hizi'].describe())

print("\nÅikayet UzunluÄŸu (Karakter SayÄ±sÄ±):")
print(df['Sikayet_Uzunlugu'].describe())

"""Kategorik DeÄŸiÅŸken DaÄŸÄ±lÄ±mlarÄ±(Ä°lÃ§e DaÄŸÄ±lÄ±mÄ±)"""

import matplotlib.pyplot as plt
import seaborn as sns

def plot_category_distribution(column_name):
    plt.figure(figsize=(10, 5))
    sns.countplot(data=df, x=column_name)
    plt.title(f'{column_name} DaÄŸÄ±lÄ±mÄ±')
    plt.xticks(rotation=45)
    plt.show()

# Kategorik sÃ¼tunlarÄ± analiz et
for column in ['Ä°lÃ§e']:
    print(f"\n{column} DeÄŸer SayÄ±larÄ±:")
    print(df[column].value_counts())
    plot_category_distribution(column)

"""Kategorik DeÄŸiÅŸken DaÄŸÄ±lÄ±mlarÄ±(Kategori)"""

for column in ['Kategori']:
    print(f"\n{column} DeÄŸer SayÄ±larÄ±:")
    print(df[column].value_counts())
    plot_category_distribution(column)

"""Kategorik DeÄŸiÅŸken DaÄŸÄ±lÄ±mlarÄ±(Kanal)"""

for column in ['Kanal']:
    print(f"\n{column} DeÄŸer SayÄ±larÄ±:")
    print(df[column].value_counts())
    plot_category_distribution(column)

"""Kategorik DeÄŸiÅŸken DaÄŸÄ±lÄ±mlarÄ±(Ã‡Ã¶zÃ¼m Durumu)"""

for column in ['Cozum_Durumu']:
    print(f"\n{column} DeÄŸer SayÄ±larÄ±:")
    print(df[column].value_counts())
    plot_category_distribution(column)

"""Kategorik DeÄŸiÅŸken DaÄŸÄ±lÄ±mlarÄ±(GÃ¼n AdÄ±)"""

for column in ['Gun_Adi']:
    print(f"\n{column} DeÄŸer SayÄ±larÄ±:")
    print(df[column].value_counts())
    plot_category_distribution(column)

"""Ã‡Ã¶zÃ¼m Durumuna GÃ¶re Analiz"""

print("\nÃ‡Ã¶zÃ¼m Durumuna GÃ¶re Ortalama Ã‡Ã¶zÃ¼m SÃ¼releri:")
print(df.groupby('Cozum_Durumu')['Cozum_Suresi_gÃ¼n'].mean())

print("\nÃ‡Ã¶zÃ¼m Durumuna GÃ¶re BaÅŸvuru SayÄ±larÄ±:")
print(df['Cozum_Durumu'].value_counts())

# Ã‡Ã¶zÃ¼lmÃ¼ÅŸ/Beklemedeki ÅŸikayetlerin oranÄ±
plt.figure(figsize=(6, 4))
df['Cozum_Durumu'].value_counts().plot.pie(autopct='%1.1f%%')
plt.title('Ã‡Ã¶zÃ¼m Durumu DaÄŸÄ±lÄ±mÄ±')
plt.show()

"""Ä°lÃ§elere GÃ¶re Performans Analizi"""

ilce_performans = df.groupby('Ä°lÃ§e').agg({
    'Cozum_Suresi_gÃ¼n': 'mean',
    'Sikayet_basvuru_sayisi': 'sum'
}).sort_values('Cozum_Suresi_gÃ¼n', ascending=True)

print("\nÄ°lÃ§elere GÃ¶re Performans Metrikleri:")
print(ilce_performans)

# Ã‡Ã¶zÃ¼m HÄ±zÄ±na GÃ¶re SÄ±ralama
plt.figure(figsize=(12, 6))
sns.barplot(data=ilce_performans.reset_index(), x='Ä°lÃ§e', y='Cozum_Suresi_gÃ¼n')
plt.title('Ä°lÃ§elere GÃ¶re Ortalama Ã‡Ã¶zÃ¼m SÃ¼resi')
plt.xticks(rotation=45)
plt.show()

"""Zaman Serisi Analizi (GÃ¼nlÃ¼k Åikayetler)"""

df['Tarih'] = pd.to_datetime(df['Tarih'])  # Tarih formatÄ±nÄ± dÃ¼zelt
df.set_index('Tarih', inplace=True)

# GÃ¼nlÃ¼k ÅŸikayet sayÄ±larÄ±
plt.figure(figsize=(12, 6))
df.resample('D')['Sikayet_basvuru_sayisi'].sum().plot()
plt.title('GÃ¼nlÃ¼k Åikayet Trendi')
plt.ylabel('Åikayet SayÄ±sÄ±')
plt.grid()
plt.show()

"""HaftanÄ±n GÃ¼nlerine GÃ¶re Analiz"""

gunluk_analiz = df.groupby('Gun_Adi').agg({
    'Sikayet_basvuru_sayisi': 'sum'
}).reindex(['Pazartesi', 'SalÄ±', 'Ã‡arÅŸamba', 'PerÅŸembe', 'Cuma', 'Cumartesi', 'Pazar'])

print("\nHaftanÄ±n GÃ¼nlerine GÃ¶re Åikayetler:")
print(gunluk_analiz)

# GÃ¶rselleÅŸtirme
plt.figure(figsize=(10, 5))
sns.lineplot(data=gunluk_analiz.reset_index(), x='Gun_Adi', y='Sikayet_basvuru_sayisi', marker='o')
plt.title('HaftanÄ±n GÃ¼nlerine GÃ¶re Åikayet SayÄ±sÄ±')
plt.show()

"""Korelasyon Analizi"""

# SayÄ±sal sÃ¼tunlar arasÄ±ndaki iliÅŸki
numeric_cols = ['Cozum_Suresi_gÃ¼n', 'Yogun_ilÃ§e', 'Sikayet_Uzunlugu']
plt.figure(figsize=(8, 6))
sns.heatmap(df[numeric_cols].corr(), annot=True, cmap='coolwarm', center=0)
plt.title('DeÄŸiÅŸkenler ArasÄ± Korelasyon')
plt.show()

"""Ã‡Ã¶zÃ¼m HÄ±zÄ±nÄ± Etkileyen FaktÃ¶rler(Kategorilere GÃ¶re Ã‡Ã¶zÃ¼m HÄ±zÄ±)"""

# Kategorilere gÃ¶re Ã§Ã¶zÃ¼m hÄ±zÄ±
plt.figure(figsize=(12, 6))
sns.boxplot(data=df, x='Kategori', y='Cozum_Hizi')
plt.xticks(rotation=45)
plt.title('Kategorilere GÃ¶re Ã‡Ã¶zÃ¼m HÄ±zÄ± DaÄŸÄ±lÄ±mÄ±')
plt.show()

"""Ã‡Ã¶zÃ¼m HÄ±zÄ±nÄ± Etkileyen FaktÃ¶rler(BaÅŸvuru KanalÄ±na GÃ¶re Ã‡Ã¶zÃ¼m HÄ±zÄ±)"""

plt.figure(figsize=(8, 5))
sns.boxplot(data=df, x='Kanal', y='Cozum_Hizi')
plt.title('KanalÄ±n Ã‡Ã¶zÃ¼m HÄ±zÄ±na Etkisi')
plt.show()

"""Tatil Etkisi Analizi"""

if 'Tatilde_Geldi_Mi' in df.columns:
    tatil_etkisi = df.groupby('Tatilde_Geldi_Mi').agg({
        'Cozum_Suresi_gÃ¼n': 'mean',
    })
    print("\nTatilde Gelen Åikayetlerin PerformansÄ±:")
    print(tatil_etkisi)

"""Label Encoding Ä°le Kategorik Veriler SayÄ±sallaÅŸtÄ±rma"""

from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
import joblib

# Ã–rnek olarak "Ä°lÃ§e" ve "Kategori" sÃ¼tunlarÄ±
label_encoders = {}

for col in ['Ä°lÃ§e', 'Kategori', 'Gun', 'Season']:
    le = LabelEncoder()
    df[col + '_Kod'] = le.fit_transform(df[col])
    label_encoders[col] = le  # encode edilmiÅŸ nesneleri saklamak iÃ§in

# Ã–rnek: "Ä°lÃ§e" kolonu â†’ "Ä°lÃ§e_Kod"
df.head()

"""One-Hot Encoding Ä°le Kategorik Veriler SayÄ±sallaÅŸtÄ±rma"""

# Ã–rnek olarak "Kategori" ve "Ä°lÃ§e" kolonlarÄ±
one_hot_df = pd.get_dummies(df, columns=['Ä°lÃ§e', 'Kategori'], prefix=['Ilce', 'Kategori'])

# SonuÃ§
one_hot_df.head()

"""Makine Ã–ÄŸrenmesi Ä°Ã§in Gerekli KÃ¼tÃ¼phanelerin YÃ¼klemesi"""

from sklearn.model_selection import train_test_split

# Ã–zellikler ve hedef deÄŸiÅŸken
X = df[['Ä°lÃ§e_Kod', 'Kategori_Kod', 'Sikayet_Uzunlugu', 'Gun_Kod', 'Season_Kod']]
y = df['Kategori_Kod']  # ya da kendi etiket sÃ¼tununa gÃ¶re deÄŸiÅŸtir

# EÄŸitim ve test kÃ¼melerini oluÅŸtur
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Naive Bayes Modeli"""

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, accuracy_score
import joblib

# Modeli oluÅŸtur ve eÄŸit
nb_model = GaussianNB()
nb_model.fit(X_train, y_train)

# Test verisi ile tahmin yap
y_pred_nb = nb_model.predict(X_test)

# SonuÃ§larÄ± gÃ¶ster
print("Naive Bayes Accuracy:", accuracy_score(y_test, y_pred_nb))
print("Classification Report:\n", classification_report(y_test, y_pred_nb))

# Tahminleri veri Ã§erÃ§evesine ekle
df['NB_Tahmin'] = nb_model.predict(X)

# Modeli .pkl dosyasÄ± olarak kaydet
joblib.dump(nb_model, 'naive_bayes_model.pkl')

"""YukarÄ±daki Naive Bayes modelinin dÃ¼ÅŸÃ¼k doÄŸruluÄŸu (%33.33) ve sÄ±nÄ±flandÄ±rma raporundaki '0.00' deÄŸerleri ile gÃ¶sterilen performans sorunlarÄ± gÃ¶z Ã¶nÃ¼nde bulundurularak, modelin performansÄ±nÄ± artÄ±rmak adÄ±na gerekli dÃ¼zenlemeler yapÄ±lmÄ±ÅŸtÄ±r."""

# Gerekli kÃ¼tÃ¼phaneleri iÃ§e aktarma
import pandas as pd
import io
import seaborn as sns
import matplotlib.pyplot as plt

# Makine Ã¶ÄŸrenmesi iÃ§in gerekli modÃ¼ller
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler # MinMaxScaler eklendi
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import ComplementNB  # Dengesiz veriler ve metin iÃ§in daha uygun bir Naive Bayes tÃ¼rÃ¼
from sklearn.metrics import classification_report, confusion_matrix

# --- 1. ADIM: VERÄ°YÄ° YÃœKLEME ---
csv_data = """ID;Tarih;Ä°lÃ§e;Kategori;Detay;Kanal;Cozum_Durumu;Cozum_Suresi_gÃ¼n;Sikayet_basvuru_sayisi;Gun_Adi;Cozum_Hizi;Cevap_Var;Ilce_Sikayet_Sirasi;Gun_Sayisi_Ay;Week;Month;Season;Yogun_ilÃ§e;Sikayet_Uzunlugu;Onceki_Sikayet_Varmi;Gecikme_Etkisi;Tatilde_Geldi_Mi
1;2025-06-01;Ã‡ukurova;Yol Sorunu;Yol Ã¼zerinde bÃ¼yÃ¼k Ã§ukur oluÅŸmuÅŸ.;e-Devlet;Ã‡Ã¶zÃ¼ldÃ¼;4;1;Pazar;0.85;1;1;30;22;Haziran;Yaz;150;30;0;0;0
2;2025-06-02;Seyhan;Ã‡Ã¶p Toplama;Ã‡Ã¶pler dÃ¼zenli toplanmÄ±yor.;Ã‡aÄŸrÄ± Merkezi;Beklemede;0;1;Pazartesi;0.50;0;2;30;23;Haziran;Yaz;200;28;1;1;0
3;2025-06-03;YÃ¼reÄŸir;GÃ¼rÃ¼ltÃ¼;Gece geÃ§ saatlerde yÃ¼ksek sesli mÃ¼zik.;Mobil Uygulama;Ã‡Ã¶zÃ¼ldÃ¼;3;1;SalÄ±;0.90;1;3;30;23;Haziran;Yaz;100;40;0;0;0
4.1;2025-06-04;Feke;Elektrik Kesintisi;2 gÃ¼ndÃ¼r elektrik yok.;e-Devlet;Ã‡Ã¶zÃ¼ldÃ¼;2;1;Ã‡arÅŸamba;0.75;1;1;30;23;Haziran;Yaz;50;23;1;1;0
5;2025-06-05;Tufanbeyli;Park ve BahÃ§eler;Parkta oturma alanlarÄ± kÄ±rÄ±k.;Ã‡aÄŸrÄ± Merkezi;Ã‡Ã¶zÃ¼ldÃ¼;5;1;PerÅŸembe;0.80;1;2;30;23;Haziran;Yaz;40;32;0;0;0
6;2025-06-06;PozantÄ±;Yol Sorunu;Asfalt Ã§atlamÄ±ÅŸ, tehlike arz ediyor.;e-Devlet;Beklemede;0;1;Cuma;0.60;0;3;30;23;Haziran;Yaz;70;38;0;0;0
7;2025-06-07;KaraisalÄ±;GÃ¼rÃ¼ltÃ¼;Ä°nÅŸaat sabah Ã§ok erken baÅŸlÄ±yor.;Mobil Uygulama;Ã‡Ã¶zÃ¼ldÃ¼;1;1;Cumartesi;0.95;1;1;30;23;Haziran;Yaz;60;35;1;1;1
8;2025-06-08;SarÄ±Ã§am;Ã‡Ã¶p Toplama;Ã‡Ã¶p konteynerleri dolu ve taÅŸmÄ±ÅŸ.;Ã‡aÄŸrÄ± Merkezi;Ã‡Ã¶zÃ¼ldÃ¼;2;1;Pazar;0.70;1;2;30;23;Haziran;Yaz;180;37;1;1;1
9;2025-06-09;Ceyhan;Elektrik Kesintisi;SÄ±k sÄ±k elektrik kesiliyor.;e-Devlet;Beklemede;0;1;Pazartesi;0.55;0;1;30;24;Haziran;Yaz;120;30;0;0;0
10;2025-06-10;YumurtalÄ±k;Su Kesintisi;3 saattir su kesik.;Mobil Uygulama;Ã‡Ã¶zÃ¼ldÃ¼;1;1;SalÄ±;0.88;1;2;30;24;Haziran;Yaz;30;25;1;1;0
11;2025-06-11;KarataÅŸ;Su Kesintisi;Sular sÄ±k kesiliyor.;Ã‡aÄŸrÄ± Merkezi;Ã‡Ã¶zÃ¼ldÃ¼;3;1;Ã‡arÅŸamba;0.78;1;3;30;24;Haziran;Yaz;35;25;0;0;0
12;2025-06-12;AladaÄŸ;Park ve BahÃ§eler;Parkta temizlik yapÄ±lmÄ±yor.;e-Devlet;Ã‡Ã¶zÃ¼ldÃ¼;2;1;PerÅŸembe;0.82;1;1;30;24;Haziran;Yaz;25;30;1;1;0
13;2025-06-13;Ä°mamoÄŸlu;GÃ¼rÃ¼ltÃ¼;Kafeler yÃ¼ksek sesle mÃ¼zik Ã§alÄ±yor.;Mobil Uygulama;Beklemede;0;1;Cuma;0.65;0;2;30;24;Haziran;Yaz;45;42;0;0;0
14;2025-06-14;Kozan;Yol Sorunu;Trafik tabelalarÄ± eksik.;Ã‡aÄŸrÄ± Merkezi;Ã‡Ã¶zÃ¼ldÃ¼;4;1;Cumartesi;0.89;1;1;30;24;Haziran;Yaz;90;28;1;1;1
15;2025-06-15;Saimbeyli;Ã‡Ã¶p Toplama;Sokaklarda koku var.;e-Devlet;Ã‡Ã¶zÃ¼ldÃ¼;3;1;Pazar;0.72;1;2;30;24;Haziran;Yaz;20;25;0;0;1
"""
df = pd.read_csv(io.StringIO(csv_data), sep=';')
df['Season'] = df['Season'].str.strip()

# --- 2. ADIM: Ã–ZELLÄ°K (X) VE HEDEF (y) DEÄÄ°ÅKENLERÄ° AYIRMA ---
# Veri sÄ±zÄ±ntÄ±sÄ± ve ID gibi gereksiz sÃ¼tunlarÄ± Ã§Ä±karÄ±yoruz
y = df['Cozum_Durumu']
X = df.drop(columns=['ID', 'Tarih', 'Cozum_Durumu', 'Cozum_Suresi_gÃ¼n', 'Cozum_Hizi', 'Cevap_Var'])

# --- 3. ADIM: DOÄRU VERÄ° BÃ–LME (EN KRÄ°TÄ°K DÃœZELTME) ---
# Veriyi %70 eÄŸitim, %30 test olarak bÃ¶lÃ¼yoruz.
# stratify=y -> Bu parametre, sÄ±nÄ±flarÄ±n ('Ã‡Ã¶zÃ¼ldÃ¼'/'Beklemede') test setinde
# de aynÄ± oranda bulunmasÄ±nÄ± garanti eder. Bu, support=0 sorununu Ã§Ã¶zer.
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.3, # Test seti iÃ§in %30 ayÄ±rarak daha gÃ¼venilir bir sonuÃ§ alalÄ±m
    random_state=42,
    stratify=y       # Ã–NCEKÄ° HATAYI DÃœZELTEN EN Ã–NEMLÄ° PARAMETRE
)

print("--- Veri Seti DoÄŸru Bir Åekilde BÃ¶lÃ¼ndÃ¼ ---")
print(f"EÄŸitim Verisi SayÄ±sÄ±: {len(X_train)}")
print(f"Test Verisi SayÄ±sÄ±: {len(X_test)}\n") # ArtÄ±k 3'ten fazla olduÄŸunu gÃ¶receÄŸiniz
print("Test Setindeki SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±:")
print(y_test.value_counts()) # Her sÄ±nÄ±ftan Ã¶rnek olduÄŸunu gÃ¶receÄŸiniz
print("-" * 45)

# --- 4. ADIM: VERÄ° Ã–N Ä°ÅLEME VE MODEL PIPELINE'I ---
# SÃ¼tunlarÄ± tiplerine gÃ¶re gruplayalÄ±m
text_features = 'Detay'
categorical_features = ['Ä°lÃ§e', 'Kategori', 'Kanal', 'Gun_Adi', 'Month', 'Season']
numerical_features = [col for col in X.columns if col not in categorical_features and col != text_features]

# Her sÃ¼tun tipi iÃ§in farklÄ± iÅŸlem uygulayan Ã¶n iÅŸlemci
preprocessor = ColumnTransformer(
    transformers=[
        ('tfidf', TfidfVectorizer(), text_features),
        ('onehot', OneHotEncoder(handle_unknown='ignore'), categorical_features),
        ('scaler', MinMaxScaler(), numerical_features) # StandardScaler yerine MinMaxScaler kullanÄ±ldÄ±
    ])

# Naive Bayes modelini pipeline'a dahil etme
# ComplementNB, Ã¶zellikle metin sÄ±nÄ±flandÄ±rma ve dengesiz veri setleri iÃ§in MultinomialNB'den daha iyi performans gÃ¶sterebilir.
nb_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', ComplementNB())
])

# --- 5. ADIM: MODELÄ° EÄÄ°TME VE DEÄERLENDÄ°RME ---
# Modeli eÄŸitme
nb_pipeline.fit(X_train, y_train)

# Test seti Ã¼zerinde tahmin yapma
y_pred_nb = nb_pipeline.predict(X_test)

# --- 6. ADIM: ANLAMLI SONUÃ‡LARI GÃ–STERME ---
print("\n--- DÃ¼zeltilmiÅŸ Naive Bayes Modeli SonuÃ§larÄ± ---")
print("\nSÄ±nÄ±flandÄ±rma Raporu:")
print(classification_report(y_test, y_pred_nb))

print("\nKarmaÅŸÄ±klÄ±k Matrisi:")
cm = confusion_matrix(y_test, y_pred_nb, labels=nb_pipeline.classes_)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=nb_pipeline.classes_, yticklabels=nb_pipeline.classes_)
plt.title('Naive Bayes KarmaÅŸÄ±klÄ±k Matrisi')
plt.ylabel('GerÃ§ek DeÄŸerler')
plt.xlabel('Tahmin Edilen DeÄŸerler')
plt.show()

"""YapÄ±lan iyileÅŸtirmeler sonucunda modelin doÄŸruluk ve sÄ±nÄ±flandÄ±rma raporu deÄŸerlerinde belirgin bir dÃ¼zelme saÄŸlanmÄ±ÅŸtÄ±r.

Karar AÄŸacÄ± (Decision Tree) Modeli
"""

from sklearn.tree import DecisionTreeClassifier

# Modeli oluÅŸtur ve eÄŸit
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)

# Test verisi ile tahmin yap
y_pred_dt = dt_model.predict(X_test)

# SonuÃ§larÄ± gÃ¶ster
print("Decision Tree Accuracy:", accuracy_score(y_test, y_pred_dt))
print("Classification Report:\n", classification_report(y_test, y_pred_dt))

# Tahminleri veri Ã§erÃ§evesine ekle
df['DT_Tahmin'] = dt_model.predict(X)

# Modeli .pkl dosyasÄ± olarak kaydet
joblib.dump(dt_model, 'decision_tree_model.pkl')

"""YukarÄ±daki Karar AÄŸacÄ± modelinin dÃ¼ÅŸÃ¼k doÄŸruluÄŸu (%33.33) ve sÄ±nÄ±flandÄ±rma raporundaki '0.00' deÄŸerleri ile gÃ¶sterilen performans sorunlarÄ± gÃ¶z Ã¶nÃ¼nde bulundurularak, modelin performansÄ±nÄ± artÄ±rmak adÄ±na gerekli dÃ¼zenlemeler yapÄ±lmÄ±ÅŸtÄ±r.

Random Forest
"""

from sklearn.ensemble import RandomForestClassifier

# Modeli oluÅŸtur
rf_model = RandomForestClassifier(random_state=42)

# Modeli eÄŸit
rf_model.fit(X_train, y_train)

# Tahmin yap
rf_predictions = rf_model.predict(X_test)

# Performans Ã¶lÃ§
print("ğŸ“Œ Random Forest Classification Report:\n")
print(classification_report(y_test, rf_predictions))

# Test verisine karÅŸÄ±lÄ±k gelen tahminleri yazdÄ±r (isteÄŸe baÄŸlÄ±)
import pandas as pd

rf_output = pd.DataFrame({
    "GerÃ§ek DeÄŸer": y_test,
    "Tahmin (RF)": rf_predictions
})
print(rf_output.head())

# CSV olarak kaydetmek istersen:
rf_output.to_csv("random_forest_tahminler.csv", index=False)

"""YukarÄ±daki Random Forest modelinin doÄŸruluÄŸu ve sÄ±nÄ±flandÄ±rma raporundaki '0.00' deÄŸerleri ile gÃ¶sterilen performans sorunlarÄ± gÃ¶z Ã¶nÃ¼nde bulundurularak, modelin performansÄ±nÄ± artÄ±rmak adÄ±na gerekli dÃ¼zenlemeler yapÄ±lmÄ±ÅŸtÄ±r.

XGBOOTS
"""

pip install xgboost

from xgboost import XGBClassifier

# Modeli oluÅŸtur
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')

# EÄŸit
xgb_model.fit(X_train, y_train)

# Tahmin
xgb_predictions = xgb_model.predict(X_test)

# Performans
print("ğŸ“Œ XGBoost Classification Report:\n")
print(classification_report(y_test, xgb_predictions))

# Test verisine karÅŸÄ±lÄ±k tahmin tablosu
xgb_output = pd.DataFrame({
    "GerÃ§ek DeÄŸer": y_test,
    "Tahmin (XGBoost)": xgb_predictions
})
print(xgb_output.head())

# CSVâ€™ye kaydetmek istersen:
xgb_output.to_csv("xgboost_tahminler.csv", index=False)

"""YukarÄ±daki XGBOOTS modelinin doÄŸruluÄŸu ve sÄ±nÄ±flandÄ±rma raporundaki '0.00' deÄŸerleri ile gÃ¶sterilen performans sorunlarÄ± gÃ¶z Ã¶nÃ¼nde bulundurularak, modelin performansÄ±nÄ± artÄ±rmak adÄ±na gerekli dÃ¼zenlemeler yapÄ±lmÄ±ÅŸtÄ±r."""

# Gerekli kÃ¼tÃ¼phaneleri iÃ§e aktarma
import pandas as pd
import io
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Makine Ã¶ÄŸrenmesi iÃ§in gerekli modÃ¼ller
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline

# SÄ±nÄ±flandÄ±rma Modelleri
from sklearn.tree import DecisionTreeClassifier # <-- YENÄ° EKLENDÄ°
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb # XGBoost'u iÃ§e aktar

# DeÄŸerlendirme Metrikleri
from sklearn.metrics import classification_report, confusion_matrix

# --- 1. ADIM: VERÄ°YÄ° YÃœKLEME ---
csv_data = """ID;Tarih;Ä°lÃ§e;Kategori;Detay;Kanal;Cozum_Durumu;Cozum_Suresi_gÃ¼n;Sikayet_basvuru_sayisi;Gun_Adi;Cozum_Hizi;Cevap_Var;Ilce_Sikayet_Sirasi;Gun_Sayisi_Ay;Week;Month;Season;Yogun_ilÃ§e;Sikayet_Uzunlugu;Onceki_Sikayet_Varmi;Gecikme_Etkisi;Tatilde_Geldi_Mi
1;2025-06-01;Ã‡ukurova;Yol Sorunu;Yol Ã¼zerinde bÃ¼yÃ¼k Ã§ukur oluÅŸmuÅŸ.;e-Devlet;Ã‡Ã¶zÃ¼ldÃ¼;4;1;Pazar;0.85;1;1;30;22;Haziran;Yaz;150;30;0;0;0
2;2025-06-02;Seyhan;Ã‡Ã¶p Toplama;Ã‡Ã¶pler dÃ¼zenli toplanmÄ±yor.;Ã‡aÄŸrÄ± Merkezi;Beklemede;0;1;Pazartesi;0.50;0;2;30;23;Haziran;Yaz;200;28;1;1;0
3;2025-06-03;YÃ¼reÄŸir;GÃ¼rÃ¼ltÃ¼;Gece geÃ§ saatlerde yÃ¼ksek sesli mÃ¼zik.;Mobil Uygulama;Ã‡Ã¶zÃ¼ldÃ¼;3;1;SalÄ±;0.90;1;3;30;23;Haziran;Yaz;100;40;0;0;0
4;2025-06-04;Feke;Elektrik Kesintisi;2 gÃ¼ndÃ¼r elektrik yok.;e-Devlet;Ã‡Ã¶zÃ¼ldÃ¼;2;1;Ã‡arÅŸamba;0.75;1;1;30;23;Haziran;Yaz;50;23;1;1;0
5;2025-06-05;Tufanbeyli;Park ve BahÃ§eler;Parkta oturma alanlarÄ± kÄ±rÄ±k.;Ã‡aÄŸrÄ± Merkezi;Ã‡Ã¶zÃ¼ldÃ¼;5;1;PerÅŸembe;0.80;1;2;30;23;Haziran;Yaz;40;32;0;0;0
6;2025-06-06;PozantÄ±;Yol Sorunu;Asfalt Ã§atlamÄ±ÅŸ, tehlike arz ediyor.;e-Devlet;Beklemede;0;1;Cuma;0.60;0;3;30;23;Haziran;Yaz;70;38;0;0;0
7;2025-06-07;KaraisalÄ±;GÃ¼rÃ¼ltÃ¼;Ä°nÅŸaat sabah Ã§ok erken baÅŸlÄ±yor.;Mobil Uygulama;Ã‡Ã¶zÃ¼ldÃ¼;1;1;Cumartesi;0.95;1;1;30;23;Haziran;Yaz;60;35;1;1;1
8;2025-06-08;SarÄ±Ã§am;Ã‡Ã¶p Toplama;Ã‡Ã¶p konteynerleri dolu ve taÅŸmÄ±ÅŸ.;Ã‡aÄŸrÄ± Merkezi;Ã‡Ã¶zÃ¼ldÃ¼;2;1;Pazar;0.70;1;2;30;23;Haziran;Yaz;180;37;1;1;1
9;2025-06-09;Ceyhan;Elektrik Kesintisi;SÄ±k sÄ±k elektrik kesiliyor.;e-Devlet;Beklemede;0;1;Pazartesi;0.55;0;1;30;24;Haziran;Yaz;120;30;0;0;0
10;2025-06-10;YumurtalÄ±k;Su Kesintisi;3 saattir su kesik.;Mobil Uygulama;Ã‡Ã¶zÃ¼ldÃ¼;1;1;SalÄ±;0.88;1;2;30;24;Haziran;Yaz;30;25;1;1;0
11;2025-06-11;KarataÅŸ;Su Kesintisi;Sular sÄ±k kesiliyor.;Ã‡aÄŸrÄ± Merkezi;Ã‡Ã¶zÃ¼ldÃ¼;3;1;Ã‡arÅŸamba;0.78;1;3;30;24;Haziran;Yaz;35;25;0;0;0
12;2025-06-12;AladaÄŸ;Park ve BahÃ§eler;Parkta temizlik yapÄ±lmÄ±yor.;e-Devlet;Ã‡Ã¶zÃ¼ldÃ¼;2;1;PerÅŸembe;0.82;1;1;30;24;Haziran;Yaz;25;30;1;1;0
13;2025-06-13;Ä°mamoÄŸlu;GÃ¼rÃ¼ltÃ¼;Kafeler yÃ¼ksek sesle mÃ¼zik Ã§alÄ±yor.;Mobil Uygulama;Beklemede;0;1;Cuma;0.65;0;2;30;24;Haziran;Yaz;45;42;0;0;0
14;2025-06-14;Kozan;Yol Sorunu;Trafik tabelalarÄ± eksik.;Ã‡aÄŸrÄ± Merkezi;Ã‡Ã¶zÃ¼ldÃ¼;4;1;Cumartesi;0.89;1;1;30;24;Haziran;Yaz;90;28;1;1;1
15;2025-06-15;Saimbeyli;Ã‡Ã¶p Toplama;Sokaklarda koku var.;e-Devlet;Ã‡Ã¶zÃ¼ldÃ¼;3;1;Pazar;0.72;1;2;30;24;Haziran;Yaz;20;25;0;0;1
"""
df = pd.read_csv(io.StringIO(csv_data), sep=';')
df['Season'] = df['Season'].str.strip()

# --- 2. ADIM: DÃœZELTÄ°LMÄ°Å Ã–ZELLÄ°K VE HEDEF AYRIMI ---
y = df['Cozum_Durumu']
X = df.drop(columns=['ID', 'Tarih', 'Cozum_Durumu', 'Cozum_Suresi_gÃ¼n', 'Cozum_Hizi', 'Cevap_Var'])
y_numeric = y.map({'Ã‡Ã¶zÃ¼ldÃ¼': 1, 'Beklemede': 0})

# --- 3. ADIM: SÃœTUNLARI GRUPLAMA (DeÄŸiÅŸiklik yok) ---
text_features = 'Detay'
categorical_features = ['Ä°lÃ§e', 'Kategori', 'Kanal', 'Gun_Adi', 'Month', 'Season']
numerical_features = [col for col in X.columns if col not in categorical_features and col != text_features]

# --- 4. ADIM: Ã–N Ä°ÅLEME PÄ°PELINE'I (DeÄŸiÅŸiklik yok) ---
preprocessor = ColumnTransformer(
    transformers=[
        ('tfidf', TfidfVectorizer(), text_features),
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),
        ('scaler', StandardScaler(), numerical_features)
    ],
    remainder='passthrough'
)

# --- 5. ADIM: VERÄ°YÄ° EÄÄ°TÄ°M VE TEST OLARAK BÃ–LME (DeÄŸiÅŸiklik yok) ---
X_train, X_test, y_train, y_test = train_test_split(
    X, y_numeric,
    test_size=0.3,
    random_state=42,
    stratify=y_numeric
)

# --- 6. ADIM: MODELLERÄ° OLUÅTURMA VE EÄÄ°TME ---

# Model 1: Karar AÄŸacÄ± (Decision Tree) <--- YENÄ° EKLENDÄ°
dt_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', DecisionTreeClassifier(
        random_state=42,
        class_weight='balanced' # Adil karÅŸÄ±laÅŸtÄ±rma iÃ§in sÄ±nÄ±f aÄŸÄ±rlÄ±ÄŸÄ± dengelendi
    ))
])
dt_pipeline.fit(X_train, y_train)
print("âœ… Karar AÄŸacÄ± Modeli EÄŸitildi.")


# Model 2: Random Forest
rf_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(
        random_state=42,
        class_weight='balanced'
    ))
])
rf_pipeline.fit(X_train, y_train)
print("âœ… Random Forest Modeli EÄŸitildi.")

# Model 3: XGBoost
sÄ±nÄ±f_oranÄ± = y_train.value_counts()[1] / y_train.value_counts()[0]
xgb_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', xgb.XGBClassifier(
        random_state=42,
        use_label_encoder=False,
        eval_metric='logloss',
        scale_pos_weight=sÄ±nÄ±f_oranÄ±
    ))
])
xgb_pipeline.fit(X_train, y_train)
print("âœ… XGBoost Modeli EÄŸitildi.")

# --- 7. ADIM: SONUÃ‡LARI KARÅILAÅTIRMALI OLARAK DEÄERLENDÄ°RME ---

print("\n" + "="*50)
print("ğŸ“Š MODEL KARÅILAÅTIRMA SONUÃ‡LARI ğŸ“Š")
print("="*50)

# Karar AÄŸacÄ± SonuÃ§larÄ± <--- YENÄ° EKLENDÄ°
y_pred_dt = dt_pipeline.predict(X_test)
print("\n---  Karar AÄŸacÄ± (Decision Tree) SÄ±nÄ±flandÄ±rma Raporu ---")
print(classification_report(y_test, y_pred_dt, target_names=['Beklemede (0)', 'Ã‡Ã¶zÃ¼ldÃ¼ (1)']))

# Random Forest SonuÃ§larÄ±
y_pred_rf = rf_pipeline.predict(X_test)
print("\n---  Random Forest SÄ±nÄ±flandÄ±rma Raporu ---")
print(classification_report(y_test, y_pred_rf, target_names=['Beklemede (0)', 'Ã‡Ã¶zÃ¼ldÃ¼ (1)']))

# XGBoost SonuÃ§larÄ±
y_pred_xgb = xgb_pipeline.predict(X_test)
print("\n--- XGBoost SÄ±nÄ±flandÄ±rma Raporu ---")
print(classification_report(y_test, y_pred_xgb, target_names=['Beklemede (0)', 'Ã‡Ã¶zÃ¼ldÃ¼ (1)']))

"""Otomasyon Sistemi YapÄ±sÄ±"""

import pandas as pd
from datetime import datetime, timedelta
import smtplib
from email.mime.text import MIMEText
import matplotlib.pyplot as plt
import seaborn as sns

class SikayetOtomasyon:
    def __init__(self, veri_yolu):
        self.veri = pd.read_json(veri_yolu) if isinstance(veri_yolu, str) else pd.DataFrame(veri_yolu)
        self.veri = self.veri.reset_index() # Reset the index to make 'Tarih' a column
        self.veri['Tarih'] = pd.to_datetime(self.veri['Tarih'])
        self.son_rapor_tarihi = datetime.now()

    def yeni_sikayet_ekle(self, yeni_sikayet):
        """Yeni ÅŸikayet ekleme fonksiyonu"""
        yeni_sikayet_df = pd.DataFrame([yeni_sikayet])
        self.veri = pd.concat([self.veri, yeni_sikayet_df], ignore_index=True)
        self.veri['Tarih'] = pd.to_datetime(self.veri['Tarih'])
        print("Yeni ÅŸikayet baÅŸarÄ±yla eklendi!")

    def cozum_surecini_guncelle(self, sikayet_id, yeni_durum, cozum_suresi=None):
        """Åikayet Ã§Ã¶zÃ¼m durumunu gÃ¼ncelleme"""
        mask = self.veri['ID'] == sikayet_id
        if not any(mask):
            print(f"{sikayet_id} ID'li ÅŸikayet bulunamadÄ±!")
            return

        self.veri.loc[mask, 'Cozum_Durumu'] = yeni_durum
        if cozum_suresi is not None:
            self.veri.loc[mask, 'Cozum_Suresi_gÃ¼n'] = cozum_suresi
        print(f"{sikayet_id} ID'li ÅŸikayet gÃ¼ncellendi!")

"""Raporlama ModÃ¼lÃ¼"""

import pandas as pd
from datetime import datetime, timedelta
import smtplib
from email.mime.text import MIMEText
import matplotlib.pyplot as plt
import seaborn as sns

class SikayetOtomasyon:
    def __init__(self, veri_yolu):
        self.veri = pd.read_json(veri_yolu) if isinstance(veri_yolu, str) else pd.DataFrame(veri_yolu)
        self.veri = self.veri.reset_index() # Reset the index to make 'Tarih' a column
        self.veri['Tarih'] = pd.to_datetime(self.veri['Tarih'])
        self.son_rapor_tarihi = datetime.now()

    def yeni_sikayet_ekle(self, yeni_sikayet):
        """Yeni ÅŸikayet ekleme fonksiyonu"""
        yeni_sikayet_df = pd.DataFrame([yeni_sikayet])
        self.veri = pd.concat([self.veri, yeni_sikayet_df], ignore_index=True)
        self.veri['Tarih'] = pd.to_datetime(self.veri['Tarih'])
        print("Yeni ÅŸikayet baÅŸarÄ±yla eklendi!")

    def cozum_surecini_guncelle(self, sikayet_id, yeni_durum, cozum_suresi=None):
        """Åikayet Ã§Ã¶zÃ¼m durumunu gÃ¼ncelleme"""
        mask = self.veri['ID'] == sikayet_id
        if not any(mask):
            print(f"{sikayet_id} ID'li ÅŸikayet bulunamadÄ±!")
            return

        self.veri.loc[mask, 'Cozum_Durumu'] = yeni_durum
        if cozum_suresi is not None:
            self.veri.loc[mask, 'Cozum_Suresi_gÃ¼n'] = cozum_suresi
        print(f"{sikayet_id} ID'li ÅŸikayet gÃ¼ncellendi!")

    def otomatik_rapor_olustur(self):
        """HaftalÄ±k otomatik rapor oluÅŸturma"""
        # Use all data in the DataFrame for the report
        guncel_veri = self.veri.copy()

        if guncel_veri.empty:
            print("Rapor oluÅŸturmak iÃ§in veri bulunamadÄ±!")
            return

        # Rapor dosyasÄ± oluÅŸtur
        rapor_adi = f"sikayet_raporu_{datetime.now().strftime('%Y%m%d')}"

        # Grafikler oluÅŸtur
        self._haftalik_grafikler_olustur(guncel_veri, rapor_adi)

        # Ä°statistikler
        istatistikler = {
            'toplam_sikayet': len(guncel_veri),
            'cozulen_sikayet': len(guncel_veri[guncel_veri['Cozum_Durumu'] == 'Ã‡Ã¶zÃ¼ldÃ¼']),
            'ortalama_cozum_suresi': guncel_veri['Cozum_Suresi_gÃ¼n'].mean(),
            'en_Ã§ok_sikayet_ilce': guncel_veri['Ä°lÃ§e'].value_counts().idxmax(),
            'en_Ã§ok_sikayet_kategori': guncel_veri['Kategori'].value_counts().idxmax()
        }

        # Raporu kaydet
        self._raporu_kaydet(rapor_adi + ".pdf", istatistikler, rapor_adi)
        self.son_rapor_tarihi = datetime.now()
        print(f"{rapor_adi}.pdf adlÄ± rapor oluÅŸturuldu!")

    def _haftalik_grafikler_olustur(self, veri, rapor_adi):
        """Rapor iÃ§in grafikler oluÅŸturur"""
        # 1. Ä°lÃ§elere gÃ¶re ÅŸikayet daÄŸÄ±lÄ±mÄ±
        plt.figure(figsize=(10, 5))
        sns.countplot(data=veri, x='Ä°lÃ§e', order=veri['Ä°lÃ§e'].value_counts().index)
        plt.title('Ä°lÃ§elere GÃ¶re Åikayet DaÄŸÄ±lÄ±mÄ±')
        plt.xticks(rotation=45)
        plt.savefig(rapor_adi + '_ilce.png')
        plt.close()

        # 2. Kategorilere gÃ¶re daÄŸÄ±lÄ±m
        plt.figure(figsize=(10, 5))
        sns.countplot(data=veri, x='Kategori', order=veri['Kategori'].value_counts().index)
        plt.title('Kategorilere GÃ¶re Åikayet DaÄŸÄ±lÄ±mÄ±')
        plt.xticks(rotation=45)
        plt.savefig(rapor_adi + '_kategori.png')
        plt.close()

        # 3. Ã‡Ã¶zÃ¼m durumu
        plt.figure(figsize=(6, 4))
        veri['Cozum_Durumu'].value_counts().plot.pie(autopct='%1.1f%%')
        plt.title('Ã‡Ã¶zÃ¼m Durumu')
        plt.savefig(rapor_adi + '_cozum_durumu.png')
        plt.close()


        # 4. Ã‡Ã¶zÃ¼m sÃ¼releri
        plt.figure(figsize=(10, 5))
        sns.histplot(data=veri, x='Cozum_Suresi_gÃ¼n', bins=10, kde=True)
        plt.title('Ã‡Ã¶zÃ¼m SÃ¼releri DaÄŸÄ±lÄ±mÄ±')
        plt.savefig(rapor_adi + '_cozum_suresi.png')
        plt.close()


    def _raporu_kaydet(self, dosya_adi, istatistikler, rapor_adi_on_ek):
        """Raporu PDF olarak kaydetme"""
        from fpdf import FPDF

        pdf = FPDF()
        pdf.add_page()
        pdf.set_font("Arial", size=12)

        # BaÅŸlÄ±k
        pdf.cell(200, 10, txt="HaftalÄ±k Åikayet Raporu", ln=1, align='C')
        pdf.cell(200, 10, txt=f"Rapor Tarihi: {datetime.now().strftime('%d.%m.%Y')}", ln=2, align='C')

        # Ä°statistikler
        pdf.cell(200, 10, txt="Temel Ä°statistikler:", ln=3)
        pdf.cell(200, 10, txt=f"- Toplam Åikayet SayÄ±sÄ±: {istatistikler['toplam_sikayet']}", ln=4)
        pdf.cell(200, 10, txt=f"- Ã‡Ã¶zÃ¼len Åikayet SayÄ±sÄ±: {istatistikler['cozulen_sikayet']}", ln=5)
        pdf.cell(200, 10, txt=f"- Ortalama Ã‡Ã¶zÃ¼m SÃ¼resi: {istatistikler['ortalama_cozum_suresi']:.1f} gÃ¼n", ln=6)
        pdf.cell(200, 10, txt=f"- En Ã‡ok Åikayet Ä°lÃ§esi: {istatistikler['en_Ã§ok_sikayet_ilÃ§e']}", ln=7)
        pdf.cell(200, 10, txt=f"- En Ã‡ok Åikayet Kategorisi: {istatistikler['en_Ã§ok_sikayet_kategori']}", ln=8)

        # Grafik ekleme
        pdf.add_page() # Add a new page for graphics
        grafik_dosyalar = [
            f"{rapor_adi_on_ek}_ilce.png",
            f"{rapor_adi_on_ek}_kategori.png",
            f"{rapor_adi_on_ek}_cozum_durumu.png",
            f"{rapor_adi_on_ek}_cozum_suresi.png"
        ]

        y_position = 10
        for grafik_dosya in grafik_dosyalar:
            try:
                pdf.image(grafik_dosya, x=10, y=y_position, w=180)
                y_position += 100 # Move down for the next image
            except Exception as e:
                print(f"Hata: {grafik_dosya} dosyasÄ± eklenemedi. {e}")
            if y_position > 250: # Check if a new page is needed
                pdf.add_page()
                y_position = 10

        pdf.output(dosya_adi)

    def uyarilari_kontrol_et(self):
        """Bekleyen ve geciken ÅŸikayetler iÃ§in uyarÄ± oluÅŸturur"""
        # Beklemedeki ÅŸikayetler
        beklemedekiler = self.veri[self.veri['Cozum_Durumu'] == 'Beklemede']

        # Ã‡Ã¶zÃ¼m sÃ¼resi uzayan ÅŸikayetler (3 gÃ¼nden fazla)
        gecikenler = self.veri[
            (self.veri['Cozum_Durumu'] == 'Ã‡Ã¶zÃ¼ldÃ¼') &
            (self.veri['Cozum_Suresi_gÃ¼n'] > 3)
        ]

        if not beklemedekiler.empty:
            print(f"UYARI: {len(beklemedekiler)} adet beklemede ÅŸikayet var!")
            for _, row in beklemedekiler.iterrows():
                print(f"ID: {row['ID']}, Ä°lÃ§e: {row['Ä°lÃ§e']}, Kategori: {row['Kategori']}")

        if not gecikenler.empty:
            print(f"\nUYARI: {len(gecikenler)} adet geciken Ã§Ã¶zÃ¼m var!")
            for _, row in gecikenler.iterrows():
                print(f"ID: {row['ID']}, Ã‡Ã¶zÃ¼m SÃ¼resi: {row['Cozum_Suresi_gÃ¼n']} gÃ¼n")

        return {
            'beklemedeki_sikayetler': len(beklemedekiler),
            'geciken_cozumler': len(gecikenler)
        }

    def email_gonder(self, alici, konu, icerik):
        """E-posta bildirimi gÃ¶nderme"""
        msg = MIMEText(icerik)
        msg['Subject'] = konu
        msg['From'] = 'sikayet.otomasyon@ornek.com'
        msg['To'] = alici

        try:
            # SMTP ayarlarÄ± (Ã¶rnektir, gerÃ§ek bilgilerle deÄŸiÅŸtirin)
            with smtplib.SMTP('smtp.ornek.com', 587) as server:
                server.starttls()
                server.login('kullanici', 'sifre')
                server.send_message(msg)
            print(f"{alici} adresine e-posta gÃ¶nderildi!")
        except Exception as e:
            print(f"E-posta gÃ¶nderilemedi! Hata: {e}")

"""UyarÄ± ve Bildirim Sistemi"""

def uyarilari_kontrol_et(self):
        """Bekleyen ve geciken ÅŸikayetler iÃ§in uyarÄ± oluÅŸturur"""
        # Beklemedeki ÅŸikayetler
        beklemedekiler = self.veri[self.veri['Cozum_Durumu'] == 'Beklemede']

        # Ã‡Ã¶zÃ¼m sÃ¼resi uzayan ÅŸikayetler (3 gÃ¼nden fazla)
        gecikenler = self.veri[
            (self.veri['Cozum_Durumu'] == 'Ã‡Ã¶zÃ¼ldÃ¼') &
            (self.veri['Cozum_Suresi_gÃ¼n'] > 3)
        ]

        if not beklemedekiler.empty:
            print(f"UYARI: {len(beklemedekiler)} adet beklemede ÅŸikayet var!")
            for _, row in beklemedekiler.iterrows():
                print(f"ID: {row['ID']}, Ä°lÃ§e: {row['Ä°lÃ§e']}, Kategori: {row['Kategori']}")

        if not gecikenler.empty:
            print(f"\nUYARI: {len(gecikenler)} adet geciken Ã§Ã¶zÃ¼m var!")
            for _, row in gecikenler.iterrows():
                print(f"ID: {row['ID']}, Ã‡Ã¶zÃ¼m SÃ¼resi: {row['Cozum_Suresi_gÃ¼n']} gÃ¼n")

        return {
            'beklemedeki_sikayetler': len(beklemedekiler),
            'geciken_cozumler': len(gecikenler)
        }

    def email_gonder(self, alici, konu, icerik):
        """E-posta bildirimi gÃ¶nderme"""
        msg = MIMEText(icerik)
        msg['Subject'] = konu
        msg['From'] = 'sikayet.otomasyon@ornek.com'
        msg['To'] = alici

        try:
            # SMTP ayarlarÄ± (Ã¶rnektir, gerÃ§ek bilgilerle deÄŸiÅŸtirin)
            with smtplib.SMTP('smtp.ornek.com', 587) as server:
                server.starttls()
                server.login('kullanici', 'sifre')
                server.send_message(msg)
            print(f"{alici} adresine e-posta gÃ¶nderildi!")
        except Exception as e:
            print(f"E-posta gÃ¶nderilemedi! Hata: {e}")

"""Gerekli KÃ¼tÃ¼phaneler"""

from fpdf import FPDF
import pandas as pd

"""PDF Raporlama Fonksiyonu"""

def create_pdf_report(df, filename="veri_raporu.pdf"):
    pdf = FPDF()
    pdf.add_page()
    pdf.set_font("Arial", size=12)

    # BaÅŸlÄ±k
    pdf.set_font("Arial", 'B', 14)
    pdf.cell(200, 10, txt="Veri Raporu", ln=True, align='C')

    # Veri seti bilgisi
    pdf.set_font("Arial", size=12)
    pdf.ln(10)
    pdf.cell(200, 10, txt=f"Toplam GÃ¶zlem SayÄ±sÄ±: {df.shape[0]}", ln=True)
    pdf.cell(200, 10, txt=f"Toplam Ã–zellik SayÄ±sÄ±: {df.shape[1]}", ln=True)

    pdf.ln(5)
    pdf.set_font("Arial", 'B', 12)
    pdf.cell(200, 10, txt="Temel Ä°statistikler:", ln=True)

    # describe() Ã§Ä±ktÄ±sÄ±nÄ± yaz
    desc = df.describe().round(2)
    for col in desc.columns:
        pdf.set_font("Arial", 'B', 11)
        pdf.cell(200, 10, txt=f"\nKolon: {col}", ln=True)
        pdf.set_font("Arial", size=11)
        for stat in desc.index:
            val = desc.loc[stat, col]
            pdf.cell(200, 8, txt=f"{stat}: {val}", ln=True)

    # Kaydet
    pdf.output(filename)
    print(f"PDF raporu oluÅŸturuldu: {filename}")

"""Entegre Ã‡alÄ±ÅŸtÄ±rma"""

import pandas as pd
from datetime import datetime, timedelta

class SikayetOtomasyon:
    def __init__(self, veri_yolu):
        self.veri = pd.read_json(veri_yolu) if isinstance(veri_yolu, str) else pd.DataFrame(veri_yolu)
        self.veri = self.veri.reset_index() # Reset the index to make 'Tarih' a column
        self.veri['Tarih'] = pd.to_datetime(self.veri['Tarih'])
        self.son_rapor_tarihi = datetime.now()

    def yeni_sikayet_ekle(self, yeni_sikayet):
        yeni_sikayet_df = pd.DataFrame([yeni_sikayet])
        self.veri = pd.concat([self.veri, yeni_sikayet_df], ignore_index=True)
        self.veri['Tarih'] = pd.to_datetime(self.veri['Tarih'])
        print("Yeni ÅŸikayet baÅŸarÄ±yla eklendi!")

    def cozum_surecini_guncelle(self, sikayet_id, yeni_durum, cozum_suresi=None):
        mask = self.veri['ID'] == sikayet_id
        if not any(mask):
            print(f"{sikayet_id} ID'li ÅŸikayet bulunamadÄ±!")
            return

        self.veri.loc[mask, 'Cozum_Durumu'] = yeni_durum
        if cozum_suresi is not None:
            self.veri.loc[mask, 'Cozum_Suresi_gÃ¼n'] = cozum_suresi
        print(f"{sikayet_id} ID'li ÅŸikayet gÃ¼ncellendi!")

    def uyarilari_kontrol_et(self):
        """Bekleyen ve geciken ÅŸikayetler iÃ§in uyarÄ± oluÅŸturur"""
        # Beklemedeki ÅŸikayetler
        beklemedekiler = self.veri[self.veri['Cozum_Durumu'] == 'Beklemede']

        # Ã‡Ã¶zÃ¼m sÃ¼resi uzayan ÅŸikayetler (3 gÃ¼nden fazla)
        gecikenler = self.veri[
            (self.veri['Cozum_Durumu'] == 'Ã‡Ã¶zÃ¼ldÃ¼') &
            (self.veri['Cozum_Suresi_gÃ¼n'] > 3)
        ]

        if not beklemedekiler.empty:
            print(f"\nUYARI: {len(beklemedekiler)} adet beklemede ÅŸikayet var!")
            for _, row in beklemedekiler.iterrows():
                print(f"ID: {row['ID']}, Ä°lÃ§e: {row['Ä°lÃ§e']}, Kategori: {row['Kategori']}")
        else:
            print("\nBeklemede ÅŸikayet bulunmamaktadÄ±r.")

        if not gecikenler.empty:
            print(f"\nUYARI: {len(gecikenler)} adet geciken Ã§Ã¶zÃ¼m var!")
            for _, row in gecikenler.iterrows():
                print(f"ID: {row['ID']}, Ã‡Ã¶zÃ¼m SÃ¼resi: {row['Cozum_Suresi_gÃ¼n']} gÃ¼n")
        else:
            print("\nGeciken Ã§Ã¶zÃ¼m bulunmamaktadÄ±r.")

        return {
            'beklemedeki_sikayetler': len(beklemedekiler),
            'geciken_cozumler': len(gecikenler)
        }

# Ã–rnek veri (sizin JSON veriniz)
data = []  # Buraya orijinal JSON verinizi ekleyin

if __name__ == "__main__":
    # Otomasyon sistemini baÅŸlat
    otomasyon = SikayetOtomasyon(df)

    # Yeni ÅŸikayet ekleme Ã¶rneÄŸi
    yeni_sikayet = {
        "ID": 16,
        "Tarih": "2025-06-16",
        "Ä°lÃ§e": "Ã‡ukurova",
        "Kategori": "Su Kesintisi",
        "Detay": "24 saattir su yok",
        "Kanal": "Mobil Uygulama",
        "Cozum_Durumu": "Beklemede",
        "Cozum_Suresi_gÃ¼n": 0,
        "Sikayet_basvuru_sayisi": 1,
        "Gun_Adi": "Pazartesi",
        "Cozum_Hizi": 0.0,
        "Cevap_Var": 0,
        "Ilce_Sikayet_Sirasi": 1,
        "Gun_Sayisi_Ay": 30,
        "Week": 25,
        "Month": "Haziran",
        "Season": "Yaz",
        "Yogun_ilÃ§e": 150,
        "Sikayet_Uzunlugu": 20,
        "Onceki_Sikayet_Varmi": 0,
        "Gecikme_Etkisi": 0,
        "Tatilde_Geldi_Mi": 0
    }
    otomasyon.yeni_sikayet_ekle(yeni_sikayet)

    # Ã‡Ã¶zÃ¼m gÃ¼ncelleme Ã¶rneÄŸi
    otomasyon.cozum_surecini_guncelle(sikayet_id=2, yeni_durum="Ã‡Ã¶zÃ¼ldÃ¼", cozum_suresi=2)

    # UyarÄ± kontrolÃ¼
    otomasyon.uyarilari_kontrol_et()

"""Web ArayÃ¼zÃ¼ Entegrasyonu"""

from flask import Flask, request, jsonify, send_file
import pandas as pd
from datetime import datetime, timedelta

# Assuming SikayetOtomasyon class is defined in a previous cell and executed
# If not, you might need to include the class definition here as well

app = Flask(__name__)
otomasyon = SikayetOtomasyon(df) # Pass the DataFrame directly

@app.route('/api/sikayetler', methods=['GET'])
def tum_sikayetler():
    return jsonify(otomasyon.veri.to_dict(orient='records'))

@app.route('/api/rapor', methods=['GET'])
def rapor_olustur():
    rapor_adi = f"rapor_{datetime.now().strftime('%Y%m%d')}.pdf"
    otomasyon.otomatik_rapor_olustur()
    # Assuming the report is saved as rapor_adi in the current directory by otomatik_rapor_olustur
    return send_file(rapor_adi, as_attachment=True)

@app.route('/api/sikayet-ekle', methods=['POST'])
def sikayet_ekle():
    yeni_sikayet = request.json
    otomasyon.yeni_sikayet_ekle(yeni_sikayet)
    return jsonify({"status": "success"})

if __name__ == '__main__':
    # When running in Colab, you need to use ngrok or a similar service for external access
    # For local testing within Colab, you can use a simple run
    # app.run(debug=True)
    # Example for ngrok (requires installing flask-ngrok: pip install flask-ngrok)
    # from flask_ngrok import run_with_ngrok
    # run_with_ngrok(app)
    # app.run()
    print("Flask app is set up. You might need to use ngrok or a similar tool to expose it.")

"""Modelini EÄŸit ve Kaydet"""

# 1_model_egit_kaydet.py
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
import joblib
from datetime import datetime

# Load the dataframe
df = pd.read_csv("Sikayet_1.csv", sep=';')

# Ensure Tarih is datetime and perform feature engineering for needed columns
df['Tarih'] = pd.to_datetime(df['Tarih'], errors='coerce') # Ensure Tarih is datetime
df["Gun_Sayisi_Ay"] = df["Tarih"].dt.day # Assuming Gun_Sayisi_Ay was day of month
df["Week"] = df["Tarih"].dt.isocalendar().week.astype(int) # Create Week feature

print("DataFrame columns before selecting features:", df.columns) # Diagnostic print

# Girdi ve hedef
X = df[["Gun_Sayisi_Ay", "Week"]] # Use 'Week' column
y = df["Cozum_Suresi_gÃ¼n"]  # Use 'Cozum_Suresi_gÃ¼n' as the target column

# Modeli eÄŸit
model = SVR()
model.fit(X, y)

# Modeli kaydet
joblib.dump(model, "model.joblib")

print("Model successfully trained and saved as model.joblib")

"""Flask Web API OluÅŸtur"""

# Flask ve pyngrok kurulumu
!pip install flask flask-ngrok

# api_app.py
from flask import Flask, request, jsonify
import pandas as pd
import os

app = Flask(__name__)
UPLOAD_FOLDER = 'uploads'
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER

@app.route('/api/upload', methods=['POST'])
def upload_file():
    if 'file' not in request.files:
        return jsonify({'error': 'Dosya bulunamadÄ±'}), 400

    file = request.files['file']
    filepath = os.path.join(app.config['UPLOAD_FOLDER'], file.filename)
    file.save(filepath)

    try:
        df = pd.read_csv(filepath)
        mean_values = df.describe().loc['mean'].to_dict()
        return jsonify({'mean_values': mean_values})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    app.run(debug=True, port=5000)

"""Tam Entegre Flask UygulamasÄ±"""

from flask import Flask, render_template_string, request
import pandas as pd
import os

app = Flask(__name__)
UPLOAD_FOLDER = 'uploads'
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER

HTML_PAGE = '''
<!doctype html>
<html>
<head>
    <title>CSV Otomasyon UygulamasÄ±</title>
</head>
<body>
    <h1>CSV DosyasÄ± YÃ¼kle</h1>
    <form action="/" method="post" enctype="multipart/form-data">
        <input type="file" name="file" accept=".csv">
        <input type="submit" value="YÃ¼kle ve Ä°ÅŸle">
    </form>
    {% if table %}
        <h2>Ä°ÅŸlenmiÅŸ Veri</h2>
        {{ table|safe }}
    {% endif %}
</body>
</html>
'''

@app.route('/', methods=['GET', 'POST'])
def upload_and_process():
    table_html = None
    if request.method == 'POST':
        file = request.files['file']
        if file.filename == '':
            return render_template_string(HTML_PAGE, table=None)
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], file.filename)
        file.save(filepath)

        # ğŸ”§ Otomasyon Ä°ÅŸlemleri
        df = pd.read_csv(filepath)

        # Ã–rnek otomasyon: Eksik verileri doldur
        df.fillna(method='ffill', inplace=True)

        # Ortalama sÃ¼tun hesapla (numerik sÃ¼tunlarda)
        df['Ortalama'] = df.select_dtypes(include='number').mean(axis=1)

        # Ã–rnek: Åikayet uzunluÄŸu (text sÃ¼tunu varsa)
        for col in df.select_dtypes(include='object'):
            df[f'{col}_Uzunluk'] = df[col].astype(str).apply(len)

        # ğŸ“Š SonuÃ§larÄ± tablo olarak HTML'e dÃ¶nÃ¼ÅŸtÃ¼r
        table_html = df.to_html(classes='data', header="true", index=False)

    return render_template_string(HTML_PAGE, table=table_html)

if __name__ == '__main__':
    app.run(debug=True)

"""Python ile ZamanlanmÄ±ÅŸ GÃ¶rev Script'i (zamanlayici.py)"""

import joblib
import pandas as pd

# Not: Bu hÃ¼crenin Ã§alÄ±ÅŸmasÄ± iÃ§in yukarÄ±daki "Makine Ã–ÄŸrenmesi Modeli GeliÅŸtirme"
# adÄ±mÄ±nÄ±n Ã§alÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ ve 'rf_model' ile 'X_train' deÄŸiÅŸkenlerinin
# oluÅŸturulmuÅŸ olmasÄ± gerekmektedir.

# 1. En iyi modelimizi kaydedelim
joblib.dump(rf_model, 'cozum_suresi_modeli.pkl')
print("Model baÅŸarÄ±yla 'cozum_suresi_modeli.pkl' olarak kaydedildi.")

# 2. Modelin eÄŸitildiÄŸi sÃ¼tunlarÄ±n listesini kaydedelim
train_columns = X_train.columns.tolist()
joblib.dump(train_columns, 'egitim_sutunlari.pkl')
print("EÄŸitim sÃ¼tunlarÄ± baÅŸarÄ±yla 'egitim_sutunlari.pkl' olarak kaydedildi.")

"""Ä°ÅŸletim Sistemi Seviyesinde Zamanlama (Task Scheduler / Cron)"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile zamanlayici.py
# import schedule
# import time
# from datetime import datetime
# import pandas as pd
# import numpy as np
# import joblib
# 
# # --- 1. Veri Ã–n Ä°ÅŸleme Fonksiyonu ---
# def on_isle(df, train_columns):
#     """Gelen ham veriyi modele uygun hale getiren fonksiyon."""
#     df['Tarih'] = pd.to_datetime(df['Tarih'])
#     df['YÄ±l'] = df['Tarih'].dt.year
#     df['Ay'] = df['Tarih'].dt.month
#     df['GÃ¼n'] = df['Tarih'].dt.day
#     df['HaftanÄ±n_GÃ¼nÃ¼'] = df['Tarih'].dt.day_name()
#     df.drop('Tarih', axis=1, inplace=True)
#     df['Detay_Karakter_SayÄ±sÄ±'] = df['Detay'].apply(len)
#     df['Detay_Kelime_SayÄ±sÄ±'] = df['Detay'].apply(lambda x: len(str(x).split()))
#     df['Detay_Cukur_Var'] = df['Detay'].apply(lambda x: 1 if 'Ã§ukur' in str(x).lower() else 0)
#     df.drop(['ID', 'Detay', 'Ã‡Ã¶zÃ¼m_sÃ¼resi_gÃ¼n'], axis=1, inplace=True, errors='ignore')
#     categorical_cols = ['Ä°lÃ§e', 'Kategori', 'Kanal', 'Ã‡Ã¶zÃ¼m_Durumu', 'HaftanÄ±n_GÃ¼nÃ¼']
#     df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
#     missing_cols = set(train_columns) - set(df.columns)
#     for c in missing_cols:
#         df[c] = 0
#     df = df[train_columns]
#     return df
# 
# # --- 2. Ana GÃ¶rev Fonksiyonu ---
# def gunluk_analiz_gorevi():
#     """GÃ¼nlÃ¼k olarak Ã§alÄ±ÅŸtÄ±rÄ±lacak ana gÃ¶rev."""
#     print(f"{datetime.now()}: GÃ¼nlÃ¼k analiz gÃ¶revi baÅŸlatÄ±ldÄ±.")
#     try:
#         df_yeni = pd.read_csv('yeni_sikayetler.csv')
#         print(f"{len(df_yeni)} adet yeni ÅŸikayet bulundu.")
#         model = joblib.load('cozum_suresi_modeli.pkl')
#         train_columns = joblib.load('egitim_sutunlari.pkl')
#         df_islenmis = on_isle(df_yeni.copy(), train_columns)
#         tahminler = model.predict(df_islenmis)
#         df_yeni['Tahmin_Edilen_Cozum_Suresi_Gun'] = np.round(tahminler, 1)
#         timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#         output_filename = f"tahmin_sonuclari_{timestamp}.csv"
#         df_yeni.to_csv(output_filename, index=False)
#         print(f"SonuÃ§lar '{output_filename}' dosyasÄ±na kaydedildi.")
#     except FileNotFoundError:
#         print("HATA: Gerekli dosyalar ('yeni_sikayetler.csv', 'cozum_suresi_modeli.pkl' veya 'egitim_sutunlari.pkl') bulunamadÄ±.")
#     except Exception as e:
#         print(f"GÃ¶rev sÄ±rasÄ±nda bir hata oluÅŸtu: {e}")
#     finally:
#         print(f"{datetime.now()}: GÃ¶rev tamamlandÄ±.\n" + "-"*40)
# 
# # --- 3. ZamanlayÄ±cÄ± Kurulumu ---
# schedule.every().day.at("09:00").do(gunluk_analiz_gorevi)
# print("ZamanlayÄ±cÄ± baÅŸlatÄ±ldÄ±. GÃ¶revlerin Ã§alÄ±ÅŸmasÄ± iÃ§in bekleniyor...")
# while True:
#     schedule.run_pending()
#     time.sleep(1)```
# **Ã‡Ä±ktÄ±sÄ±:**

"""Ek ModÃ¼l: TensorFlow/Keras ile Åikayet Metinlerinden Kategori Tahmini

Metin Verisini Ã–n Ä°ÅŸleme
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt

# --- 1. Veri Setini YÃ¼kleme ---
# Use the existing DataFrame 'df'
df_nlp = df.copy() # Use a copy to avoid modifying the original df directly if needed later

# GiriÅŸ Ã¶zelliklerini (X) ve hedef deÄŸiÅŸkeni (y) ayÄ±rÄ±n
metinler = df_nlp['Detay'].values # Use 'Detay' column for text
etiketler_str = df_nlp['Kategori'].values # Use 'Kategori' column for labels

# EÄŸer etiketleriniz metin ise, sayÄ±sal etiketlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼n
label_encoder = LabelEncoder()
etiketler = label_encoder.fit_transform(etiketler_str) # y_text'i sayÄ±sal etiketlere dÃ¶nÃ¼ÅŸtÃ¼r

# SÄ±nÄ±f sayÄ±sÄ±nÄ± belirleyin
num_classes = len(np.unique(etiketler))

# --- DiÄŸer kÄ±sÄ±mlar yukarÄ±daki Ã¶rnekle aynÄ± kalÄ±r ---
# 2. Derin Ã–ÄŸrenme Modeli OluÅŸturma
# 3. Modeli Derleme
# 4. Modeli EÄŸitme
# 5. PerformansÄ± GÃ¶rselleÅŸtirme
# 6. Modelin DeÄŸerlendirilmesi (X_test_nlp, y_test_nlp ile)

Model EÄŸitimi ve DeÄŸerlendirme

# 3.1. Modeli EÄŸitme
num_epochs = 20 # Deneme amaÃ§lÄ± az epoch ile baÅŸlayabiliriz, daha sonra artÄ±rÄ±labilir
batch_size = 32

print("\nModel EÄŸitimi BaÅŸlÄ±yor...")
# Train the model using the single input text data and labels
history = model_dl.fit(
    X_train_nlp,  # Use the preprocessed text training data
    y_train_nlp,  # Use the corresponding labels
    epochs=num_epochs,
    batch_size=batch_size,
    validation_split=0.1, # EÄŸitim setinin %10'u doÄŸrulama iÃ§in kullanÄ±lacak
    verbose=1
)
print("Model EÄŸitimi TamamlandÄ±.")

# 3.2. PerformansÄ± GÃ¶rselleÅŸtirme
plt.figure(figsize=(12, 5))

# KayÄ±p (Loss) GrafiÄŸi
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='EÄŸitim KaybÄ±')
plt.plot(history.history['val_loss'], label='DoÄŸrulama KaybÄ±')
plt.title('Model KaybÄ± (Loss)')
plt.xlabel('Epoch')
plt.ylabel('KayÄ±p')
plt.legend()
plt.grid(True)

# DoÄŸruluk (Accuracy) GrafiÄŸi
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='EÄŸitim DoÄŸruluÄŸu')
plt.plot(history.history['val_accuracy'], label='DoÄŸrulama DoÄŸruluÄŸu')
plt.title('Model DoÄŸruluÄŸu (Accuracy)')
plt.xlabel('Epoch')
plt.ylabel('DoÄŸruluk')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# 3.3. Modelin Test Seti Ãœzerinde DeÄŸerlendirilmesi
print("\nModelin Test Seti Ãœzerinde DeÄŸerlendirilmesi:")
test_results = model_dl.evaluate(
    X_test_nlp,  # Use the preprocessed text test data
    y_test_nlp,  # Use the corresponding labels
    verbose=0
)
print(f"Test KaybÄ±: {test_results[0]:.4f}")
print(f"Test DoÄŸruluÄŸu: {test_results[1]:.4f}")

# Tahmin yapma (Example based on single input)
# predictions = model_dl.predict(X_test_nlp)
# predicted_classes = np.argmax(predictions, axis=1) # For multi-class classification
# print("\nÄ°lk 10 test tahmini:")
# print(predicted_classes[:10])
# print("Ä°lk 10 gerÃ§ek etiket:")
# print(y_test_nlp[:10])

""" Derin Ã–ÄŸrenme Modelini OluÅŸturma"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout # Import Dropout

# Assuming maxlen, vocab_size, embedding_dim are defined in the preceding cells

# 2.1. Model Mimarisini OluÅŸturma

# Model OluÅŸturma (Basit Bir Model) - Tek GiriÅŸli
model_dl = Sequential([
    Embedding(vocab_size, embedding_dim, input_length=max_len),
    GlobalAveragePooling1D(), # veya Flatten() veya diÄŸer pooling katmanlarÄ±
    Dense(64, activation='relu'), # Ä°lk yoÄŸun katman
    Dropout(0.3), # Overfitting'i azaltmak iÃ§in dropout
    Dense(32, activation='relu'),  # Ä°kinci yoÄŸun katman
    Dropout(0.3), # Overfitting'i azaltmak iÃ§in dropout
    Dense(num_classes, activation='softmax') # Ã‡Ä±kÄ±ÅŸ katmanÄ±: Ã‡ok sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma iÃ§in softmax
])


# 2.2. Modeli Derleme
# KayÄ±p fonksiyonu: 'sparse_categorical_crossentropy' etiketler tamsayÄ± olduÄŸunda kullanÄ±lÄ±r.
# Optimizasyon algoritmasÄ±: 'adam' yaygÄ±n ve etkili bir seÃ§enektir.
# Metrikler: 'accuracy' modelin doÄŸruluk performansÄ±nÄ± gÃ¶sterir.
model_dl.compile(optimizer='adam',
                 loss='sparse_categorical_crossentropy',
                 metrics=['accuracy'])

print("\nDerin Ã–ÄŸrenme Modeli OluÅŸturuldu ve Derlendi.")
model_dl.summary()

"""Model DeÄŸerlendirme: KonfÃ¼zyon Matrisi"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
import matplotlib.pyplot as plt
import numpy as np # np import edildiÄŸinden emin olun

"""Model Tahminleri ve KonfÃ¼zyon Matrisi Hesaplama"""

# --- Modelden Tahminleri Alma (Bu kÄ±sÄ±m sizde zaten Ã§alÄ±ÅŸÄ±yor olmalÄ±) ---
# Ã–rneÄŸin:
# predictions = model_dl.predict({'text_input': X_text_test, 'other_features_input': X_other_test})
# predicted_classes = (predictions > 0.5).astype(int) # Ä°kili sÄ±nÄ±flandÄ±rma iÃ§in olasÄ±lÄ±klarÄ± 0 veya 1'e Ã§evirme

# GerÃ§ek etiketleri (y_test) ve tahmin edilen etiketleri (predicted_classes) kullanarak KonfÃ¼zyon Matrisini oluÅŸturma
cm = confusion_matrix(y_test, predicted_classes)

"""Test Seti Ãœzerinde Tahminler ve KonfÃ¼zyon Matrisi OluÅŸturma"""

# label_encoder_cozum nesnesinin doÄŸru etiketleri iÃ§erdiÄŸinden emin olun
# (Ã–nceki kodumuzda 'Beklemede' ve 'Ã‡Ã¶zÃ¼ldÃ¼' olarak eÅŸleÅŸiyordu)
class_names = label_encoder.classes_

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Flatten, Dropout
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report

# --- Kodu BaÅŸtan Ã‡alÄ±ÅŸtÄ±rmak Ä°Ã§in Gerekli Ã–n HazÄ±rlÄ±k (Eksikse ekleyin, var olanÄ± tekrar Ã§alÄ±ÅŸtÄ±rmayÄ±nÄ±z) ---
# Genellikle Jupyter/Colab ortamÄ±nda bir kere Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda deÄŸiÅŸkenler bellekte kalÄ±r.
# EÄŸer bu notebook'u yeni baÅŸlattÄ±ysanÄ±z, Ã¶nceki veri yÃ¼kleme, Ã¶n iÅŸleme ve model oluÅŸturma
# adÄ±mlarÄ±nÄ± da tekrar Ã§alÄ±ÅŸtÄ±rmanÄ±z gerekebilir.

try:
    df = pd.read_csv('Sikayet_1.csv', delimiter=';')
except FileNotFoundError:
    print("Hata: 'Sikayet_1.csv' dosyasÄ± bulunamadÄ±. LÃ¼tfen dosyanÄ±n doÄŸru yolda olduÄŸundan emin olun.")
    exit()

label_encoder_cozum = LabelEncoder()
df['Cozum_Durumu_Encoded'] = label_encoder_cozum.fit_transform(df['Cozum_Durumu'])

# Metin Ã¶n iÅŸleme iÃ§in:
df['Detay'] = df['Detay'].fillna('')
vocab_size = 10000
maxlen = 100
tokenizer = Tokenizer(num_words=vocab_size, oov_token="<unk>")
tokenizer.fit_on_texts(df['Detay'])
padded_sequences = pad_sequences(tokenizer.texts_to_sequences(df['Detay']), maxlen=maxlen, padding='post', truncating='post')

# DiÄŸer Ã¶zelliklerin Ã¶n iÅŸlenmesi (eski koddan kopyala yapÄ±ÅŸtÄ±r)
categorical_cols = ['Ä°lÃ§e', 'Kategori', 'Kanal', 'Gun_Adi', 'Month', 'Season']
for col in categorical_cols:
    if df[col].isnull().any():
        df[col] = df[col].fillna('Yok')
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
encoded_df = pd.DataFrame(encoder.fit_transform(df[categorical_cols]), columns=encoder.get_feature_names_out(categorical_cols))

numerical_cols = ['Cozum_Suresi_gÃ¼n', 'Sikayet_basvuru_sayisi', 'Cozum_Hizi', 'Cevap_Var',
                  'Ilce_Sikayet_Sirasi', 'Gun_Sayisi_Ay', 'Week', 'Sikayet_Uzunlugu',
                  'Onceki_Sikayet_Varmi', 'Gecikme_Etkisi', 'Tatilde_Geldi_Mi']
for col in numerical_cols:
    if df[col].isnull().any():
        df[col] = df[col].fillna(df[col].median())
scaler = StandardScaler()
scaled_df = pd.DataFrame(scaler.fit_transform(df[numerical_cols]), columns=numerical_cols)

other_features = pd.concat([encoded_df, scaled_df], axis=1).fillna(0)

X_text = padded_sequences
X_other = other_features.values
y = df['Cozum_Durumu_Encoded'].values

X_text_train, X_text_test, X_other_train, X_other_test, y_train, y_test = train_test_split(
    X_text, X_other, y, test_size=0.2, random_state=42, stratify=y
)

# --- Model TanÄ±mlama ve Derleme (Ã–nceki koddan kopyala yapÄ±ÅŸtÄ±r) ---
embedding_dim = 100 # Kelime gÃ¶mme boyutu
text_input = Input(shape=(maxlen,), name='text_input')
x_text = Embedding(vocab_size, embedding_dim, input_length=maxlen)(text_input)
x_text = LSTM(128)(x_text)
x_text = Dropout(0.5)(x_text)

other_features_input = Input(shape=(X_other_train.shape[1],), name='other_features_input')
x_other = Dense(64, activation='relu')(other_features_input)
x_other = Dropout(0.3)(x_other)
x_other = Dense(32, activation='relu')(x_other)

merged = Concatenate()([x_text, x_other])
z = Dense(64, activation='relu')(merged)
z = Dropout(0.3)(z)
output = Dense(1, activation='sigmoid', name='output')(z)
model_dl = Model(inputs=[text_input, other_features_input], outputs=output)

model_dl.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# --- Modeli EÄŸitme (Ã–nceki koddan kopyala yapÄ±ÅŸtÄ±r) ---
num_epochs = 20
batch_size = 32
# history = model_dl.fit(
#     {'text_input': X_text_train, 'other_features_input': X_other_train},
#     y_train,
#     epochs=num_epochs,
#     batch_size=batch_size,
#     validation_split=0.1,
#     verbose=0 # verbose=0 eÄŸitimin Ã§Ä±ktÄ±larÄ±nÄ± gÃ¶stermez, daha temiz durur
# )

# !!! DÄ°KKAT: EÄŸer notebook'u baÅŸtan baÅŸlattÄ±ysanÄ±z, modelin eÄŸitilmesi gerekir!
# EÄŸer model zaten eÄŸitilmiÅŸse ve predictions deÄŸiÅŸkeni mevcutsa, bu satÄ±rÄ± yorum satÄ±rÄ± yapabilirsiniz.
# Aksi takdirde, hata almamak iÃ§in modeli burada eÄŸitin:
print("Model EÄŸitiliyor (eÄŸer daha Ã¶nce eÄŸitilmediyse)...")
model_dl.fit(
    {'text_input': X_text_train, 'other_features_input': X_other_train},
    y_train,
    epochs=1, # Sadece tahmin yapmak iÃ§in 1 epoch yeterli olabilir, gerÃ§ek eÄŸitim iÃ§in daha fazla yapÄ±n
    batch_size=batch_size,
    verbose=0
)
print("Model EÄŸitimi TamamlandÄ±.")

# --- Tahminleri Yapma (Bu kÄ±sÄ±m sizde olmalÄ±, burada garanti olsun diye ekledim) ---
predictions = model_dl.predict({'text_input': X_text_test, 'other_features_input': X_other_test})
predicted_classes = (predictions > 0.5).astype(int)

# --- TeÅŸhis Kod BloÄŸu: HatayÄ± Bulmak Ä°Ã§in DeÄŸiÅŸkenleri YazdÄ±rma ---
print("\n--- KonfÃ¼zyon Matrisi Hata TeÅŸhisi ---")
print(f"y_test'deki benzersiz deÄŸerler: {np.unique(y_test)}")
print(f"predicted_classes'deki benzersiz deÄŸerler: {np.unique(predicted_classes)}")
print(f"label_encoder_cozum.classes_ iÃ§eriÄŸi: {list(label_encoder_cozum.classes_)}")
print(f"label_encoder_cozum.classes_ sayÄ±sÄ±: {len(label_encoder_cozum.classes_)}")

# cm'yi tekrar oluÅŸturun, yukarÄ±daki kontrol sonuÃ§larÄ±na gÃ¶re
cm = confusion_matrix(y_test, predicted_classes)
print(f"OluÅŸturulan KonfÃ¼zyon Matrisinin Åekli (cm.shape): {cm.shape}")
print(f"KonfÃ¼zyon Matrisi (cm):\n{cm}")

# --- KonfÃ¼zyon Matrisini GÃ¶rselleÅŸtirme (DÃ¼zeltilmiÅŸ ve Kontrol EdilmiÅŸ) ---
class_names = label_encoder_cozum.classes_ # DoÄŸru LabelEncoder'dan sÄ±nÄ±flarÄ± alÄ±n
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap=plt.cm.Blues, values_format='d')
plt.title('Test Seti Ãœzerinde KonfÃ¼zyon Matrisi')
plt.show()

print("\nSÄ±nÄ±flandÄ±rma Raporu:")
print(classification_report(y_test, predicted_classes, target_names=class_names))